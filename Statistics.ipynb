{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40a91c4-02d0-4a22-a640-39ca15587b90",
   "metadata": {},
   "source": [
    "Statistics : statistics is a branch of mathematics which deals with collecting analyzing and organizing data for business purpose or decision making process.It involves a range of methods and techniques used to understand and draw conclusions from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a34c7-34cb-4c12-8042-b3aa816fbe74",
   "metadata": {},
   "source": [
    "Type's of statistics\n",
    "1. Descriptive : It is a method of organizing and summarizing data from featured dataset.\n",
    "ex: measure of tendency,measure of dispersion\n",
    "\n",
    "2. Inferential: This involves making predictions or inferences about a population based on a sample data.\n",
    "ex: Probability,hypothesis testing ,Confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff32ab-b936-4d1b-97e5-0189f9aa4e23",
   "metadata": {},
   "source": [
    "Population(N):A population refers to the entire group of individuals or observations that you are interested in studying.\n",
    "ex: All the students in a particular school.\n",
    "\n",
    "Sample(n):A sample is a subset of population that we used to make prediction or conclusion on it.\n",
    "ex:50 students randomly chosen from a school with 500 students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b5682-6dff-4583-9ca2-d60044dfa53e",
   "metadata": {},
   "source": [
    "Data: It refers to the information collected for analysis\n",
    "\n",
    "Data Collection: Methods for gathering data, including surveys, experiments, observational studies, and administrative records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bff21-dfb0-4054-94d7-aa8eb506204d",
   "metadata": {},
   "source": [
    "Types of Data Based on Measurement Levels\n",
    "\n",
    "1. Quantitative Data: This type of data represents quantities and is numerical value. It can be measured and counted.\n",
    "\n",
    "a.Discrete data:Consists of distinct, separate values. It is countable and often represents whole numbers.\n",
    "ex:Number of students in a class, number of cars in a parking lot.\n",
    "\n",
    "b.Continuous data: Can take any value within a given range. It is measurable and can be represented by fractions and decimals.\n",
    "ex; heigh,weight\n",
    "\n",
    "2. Qualitative Data: This type of data represents qualities or characteristics. It is non-numerical and is used to categorize or describe attributes of a population.\n",
    "\n",
    "a.Nominal Data: Categorical data with no inherent order or ranking. It consists of names, labels, or categories.\n",
    "ex:Gender (male, female), eye color (blue, green, brown)\n",
    "\n",
    "b.Ordinal Data: Categorical data with a meaningful order or ranking, but the intervals between values are not necessarily equal.\n",
    "ex:Educational level (high school, bachelor‚Äôs, master‚Äôs), satisfaction rating (satisfied, neutral, dissatisfied)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ea255-8346-4505-9e88-1504d1b06d07",
   "metadata": {},
   "source": [
    "DESCRIPTIVE STATITICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313a023-567e-432c-b17b-791bff24942a",
   "metadata": {},
   "source": [
    "Measure of central tendency:\n",
    "Measures of central tendency are statistical metrics that describe the center or typical value of a dataset. These measures help to summarize a large amount of data with a single value representing the middle or average of the dataset. The three most common measures of central tendency are the mean, median, and mode.\n",
    "\n",
    "1. Mean:(Average):The mean, also known as the average, is calculated by summing all the values in a dataset and dividing by the number of values.\n",
    "Mean=‚àëx/i\n",
    "where: ‚àëùë•ùëñ is the sum of all data points\n",
    "       ùëõ is the number of data points\n",
    "\n",
    "2. Median:The median is the middle value in a dataset when it is ordered from least to greatest. If there is an even number of observations, the median is the average of the two middle numbers.\n",
    "\n",
    "3. Mode:The mode is the value that appears most frequently in a dataset. A dataset may have one mode, more than one mode, or no mode at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21f8cf71-76b2-4a4d-9654-322d62be6d35",
   "metadata": {},
   "source": [
    "Measure of dispersion:\n",
    "Measures of dispersion, also known as measures of variability or spread, describe the extent to which data points in a dataset differ from the central tendency (mean, median, or mode). These measures provide insights into the distribution and variability of the data. Common measures of dispersion include the range, variance, standard deviation, interquartile range (IQR), and mean absolute deviation (MAD).\n",
    "\n",
    "1. Range:The range is the simplest measure of dispersion and represents the difference between the highest and lowest values in a dataset.\n",
    "Formula:  Range=Maximum¬†Value‚àíMinimum¬†Value\n",
    "\n",
    "2. Variance: Variance measures the average squared deviation of each data point from the mean. It provides insight into the spread of the data around the mean.\n",
    "Variance(œÉ2)=‚àë(xi‚àíŒº)2/n for sample data sample mean \n",
    "\n",
    "3. Standard Deviation: The standard deviation is the square root of the variance and provides a measure of the average distance of each data point from the mean.\n",
    "\n",
    "4. Interquartile :It's a statistical measure used to describe the spread or dispersion of a dataset by focusing on the range within which the middle 50% of the values lie. and is the difference between the first quartile (Q1) and the third quartile (Q3).\n",
    "IQR=Q3-Q1\n",
    "\n",
    "5. Mean Absolute Deviation (MAD): The mean absolute deviation is the average of the absolute deviations of each data point from the mean.\n",
    "\n",
    "NOTE : Why we divide sample variance by n-1: The sample variance is divided by n-1 so tht we can create an unbaised estimator of the population variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e672e4e-0fe5-4294-800b-ca088489b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 196.42857142857142\n",
      "Median: 200.0\n",
      "Mode: 180\n",
      "Variance: 1465.8163265306123\n",
      "Standard Deviation: 38.28598080930685\n",
      "count     14.000000\n",
      "mean     196.428571\n",
      "std       39.731240\n",
      "min      100.000000\n",
      "25%      180.000000\n",
      "50%      200.000000\n",
      "75%      227.500000\n",
      "max      250.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRJAWAL\\AppData\\Local\\Temp\\ipykernel_10120\\2517413424.py:17: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode_sales = stats.mode(sales_data)[0][0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Example data\n",
    "sales_data = pd.Series([100, 150, 230, 180, 200, 190, 210, 180, 220, 250, 170, 200, 230, 240])\n",
    "\n",
    "# Calculate mean\n",
    "mean_sales = np.mean(sales_data)\n",
    "print(f\"Mean: {mean_sales}\")\n",
    "\n",
    "# Calculate median\n",
    "median_sales = np.median(sales_data)\n",
    "print(f\"Median: {median_sales}\")\n",
    "\n",
    "# Calculate mode\n",
    "mode_sales = stats.mode(sales_data)[0][0]\n",
    "print(f\"Mode: {mode_sales}\")\n",
    "\n",
    "# Calculate variance\n",
    "variance_sales = np.var(sales_data)\n",
    "print(f\"Variance: {variance_sales}\")\n",
    "\n",
    "# Calculate standard deviation\n",
    "std_dev_sales = np.std(sales_data)\n",
    "print(f\"Standard Deviation: {std_dev_sales}\")\n",
    "\n",
    "print(sales_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481c462-606b-41f2-a3c2-96090de28a30",
   "metadata": {},
   "source": [
    "Percentile : A percentile is a measure used in statistics to indicate the value below which a given percentage of observations in a dataset falls. Percentiles are useful for understanding the distribution of data and identifying the relative standing of a particular value within a dataset.It is a value below which a certain percentage of observation lies..\n",
    "\n",
    "Percentile(number) ranking : number of values below x /n * 100\n",
    "Ranking Value = percentile/100 *(n+1)=which gives index position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b895642-6397-4ec8-aaab-a49cba1e04ce",
   "metadata": {},
   "source": [
    "Outliers: Outliers are data points that are significantly different from the rest of the dataset\n",
    "\n",
    "Handling Outlier : Using Interquartile range method\n",
    "\n",
    "IQR=Q3-Q1\n",
    "Lower bound = Q1-1.5*IQR\n",
    "Uppper bound =Q3-1.5*IQR\n",
    "\n",
    "the values between below the lower bound and above the upper bound are considered as outlier.\n",
    "\n",
    "NOTE:Quantile and percentile both are same.\n",
    "\n",
    "Using Boxplot outlier can be detected.\n",
    "Five number summary\n",
    "Minimum=0\n",
    "Q1=25%\n",
    "Median=50%\n",
    "Q3=75%\n",
    "Maximum=100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808c0af1-aa04-4fc5-b7cc-45a4fcfbbfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 40.0\n",
      "Q3: 90.0\n",
      "IQR: 50.0\n",
      "Lower Bound: -35.0\n",
      "Upper Bound: 165.0\n",
      "Outliers: [200]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Sample data\n",
    "data = [15, 20, 35, 40, 50, 55, 60, 70, 80, 90, 100, 150, 200]\n",
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = np.quantile(data, 0.25)\n",
    "Q3 = np.quantile(data, 0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Calculate lower and upper bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "# Identify outliers\n",
    "outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
    "print(\"Q1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"IQR:\", IQR)\n",
    "print(\"Lower Bound:\", lower_bound)\n",
    "print(\"Upper Bound:\", upper_bound)\n",
    "print(\"Outliers:\", outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4192204-c43f-4061-838f-fc6982f09813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset:\n",
    "num_cols = data.select_dtypes(include=[float, int]).columns\n",
    "cat_cols = data.select_dtypes(include=[object, 'category']).columns\n",
    "# Outlier removal \n",
    "# Detect outliers using IQR method\n",
    "def outlie(data):\n",
    "    Q1=data[num_cols].quantile(0.25)\n",
    "    Q3=data[num_cols].quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    outlier=((data[num_cols])<Q1-1.5*IQR)| (data[num_cols]>Q3+1.5*IQR)\n",
    "    return outlier\n",
    " # Apply the function to the dataframe   \n",
    "outlier=outlie(data)\n",
    "# Print outliers summary\n",
    "print(outlier.sum()/len(data)*100)\n",
    "\n",
    "\n",
    "data = data[~outlier.any(axis=1)] # Outlier removal \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264c2cf-8d91-4c8f-918f-d27c3e57d93f",
   "metadata": {},
   "source": [
    "Skewness : Skewness measures the asymmetry of the probability distribution of a real-valued random variable about its mean. In simpler terms, it indicates whether the data is skewed to the left (negative skew) or to the right (positive skew) relative to the normal distribution.\n",
    "\n",
    "Negative Skewness: The left tail is longer, and the mass of the distribution is concentrated on the right side. The mean is less than the median.\n",
    "\n",
    "Positive Skewness: The right tail is longer, and the mass of the distribution is concentrated on the left side. The mean is greater than the median.\n",
    "\n",
    "Zero Skewness: The distribution is symmetric around the mean (bell-shaped like the normal distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ee259-945f-475c-95b1-1ef47c3544ce",
   "metadata": {},
   "source": [
    "Kurtosis : Kurtosis is a statistical measure that describes the shape of s distribution's tails in relation to its overall shape.It indicates whether data points are more or less concentrated in the tails compared to a normal distribution.High kurtosis signifies heavy tails and possibly outliers,while low kurtosis indicates light tails and a lack of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d9f45c-4dcd-4a17-950d-56678f19cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness: 0.010249837974793676\n",
      "Kurtosis: -1.0388720391880104\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Example data\n",
    "data = np.array([15, 20, 35, 40, 50, 55, 60, 70, 80, 90])\n",
    "\n",
    "# Calculate skewness\n",
    "data_skew = skew(data)\n",
    "print(f\"Skewness: {data_skew}\")\n",
    "\n",
    "# Calculate kurtosis\n",
    "data_kurtosis = kurtosis(data)\n",
    "print(f\"Kurtosis: {data_kurtosis}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a146f16-8b96-4f76-be63-d02ec8e35a2c",
   "metadata": {},
   "source": [
    "Central Limit Theorem:\n",
    "It says that the sampling distribution of the mean will always be normally distributed as long as the sample size is large enough.Regardless of whether the population has a normal,possion binomial r any distribution , the sampling distribution of the mean will be always normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f4d5f-a4f6-4358-9e6c-fb7d1788f435",
   "metadata": {},
   "source": [
    "Z_score : It is a statistical measurement that describes to value position relative to the mean of a group of value. also known as the standard score, is a statistical measure that indicates how many standard deviations a data point is from the mean of the dataset. It helps in understanding where a particular data point stands in relation to the mean and how typical or atypical it is compared to the rest of the data.\n",
    "\n",
    "Z=(X-mean)/std\n",
    "\n",
    "Use of Z-score\n",
    "\n",
    "->Outlier Detection: Z-score helps identify outliers in a dataset. Typically, data points with a Z-score greater than 3 or less than -3 are considered outliers.\n",
    "\n",
    "->Normalization: Z-score normalization (standardization) transforms data so that it has a mean of 0 and a standard deviation of 1. This is useful in comparing data from different distributions.\n",
    "\n",
    "->Hypothesis Testing: Z-scores are used in hypothesis testing to determine whether a sample mean is significantly different from a population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458bd795-d631-4d5f-a53f-fb4c1eb10d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 51.5\n",
      "Standard Deviation: 23.350588857671234\n",
      "Z-score for 70: 0.7922712404711928\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "data = np.array([15, 20, 35, 40, 50, 55, 60, 70, 80, 90])\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(data)\n",
    "std_dev = np.std(data)\n",
    "\n",
    "# Example data point\n",
    "X = 70\n",
    "\n",
    "# Calculate Z-score\n",
    "Z = (X - mean) / std_dev\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard Deviation: {std_dev}\")\n",
    "print(f\"Z-score for {X}: {Z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b130c-39da-4786-8a78-b4f46c376b27",
   "metadata": {},
   "source": [
    "INFERENTIAL STATISTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000b622-a7d8-419b-95d8-b479e76f64c6",
   "metadata": {},
   "source": [
    "P_value: The p value is a number calculated from a statistical test,that describes how likely you are to have found a particuler set of observation if the null hypothesis were true.It is used in hypothesis testing to decide whether to reject r accept the null hypothesis.\n",
    "It is evidence against a null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc330c5-ad41-487b-9d72-d2608081530b",
   "metadata": {},
   "source": [
    "Confidence interval :It is  a range of values that is used to estimate the true value of a population paramter with certian level of confidence.It provides a measure of the uncertainty or margin of error associated with a sample statistic,such as the sample mean or proportion. \n",
    "\n",
    "p_value=1-confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfc726-0775-4371-b113-f1946a11bd2c",
   "metadata": {},
   "source": [
    "Parametric test : Parametric test are statistical methods that make assumption that the data follows normal distribution.\n",
    "\n",
    "Z_test & T_test : Both are statistical tests used to determine whether their is a significant difference between sample mean  and population mean in hypothesis testing.\n",
    "\n",
    "z_test : used for large sample with a known population standard deviation.It is based on the standard normal distribution (Z-distribution)\n",
    "1. one sample Z-test : compares the mean of single sample to known population mean.\n",
    "2. two sample Z-test : compares the mean of two independent samples to determine if they are significantly different.\n",
    "\n",
    "T_test : used for small sample or when the population standard deviation is unknown\n",
    "1. one sample t-test : compare the mean of a single sample to the mean of the population.\n",
    "2. two sample t-test : compare the means of two independent samples.\n",
    "\n",
    "Annova : Analysis of variance is a statistical method used to compare the mean of 2 or more groups to determine if there are statistically significance difference among them.\n",
    "H0 : Assumes the mean of all groups are equall\n",
    "H1 : Assumes that at least one group mean is different from the others.\n",
    "\n",
    "1. One-way-Anova : Tests the effect of a single factor (independent variable) on a dependent variable.\n",
    "2. Two-way-Anova : Tests the effect of two factors (independent variables) on a dependent variable, and also examines the interaction between the factors.\n",
    "\n",
    "Non-Parametric Test : Non-parametric test does not make assumptions about the distribution of the data.Thy are useful when data does not meet the assumptions required for parametric test.\n",
    "\n",
    "chi-square test:Test is a statistical test used to determine if there is a significant association between categorical variables. .It is non parametric test that is performed on categorical(Ordinal,Nomial) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d5859-a9af-4f39-a027-fa6ceaf48120",
   "metadata": {},
   "source": [
    "Hypothesis testing : Hypothesis Testing is a statistical method used to make inferences or draw conclusions about a population based on sample data. It involves formulating and testing a hypothesis to determine if there is enough evidence to support a particular claim or assumption.\n",
    "        \n",
    "1. Null Hypothesis (H0):\n",
    "->The default assumption or status quo that there is no effect, difference, or relationship.\n",
    "Example: \"There is no difference in test scores between two teaching methods.\"\n",
    "\n",
    "2. Alternative Hypothesis (H1 or Ha):\n",
    "->The hypothesis that contradicts the null hypothesis, indicating the presence of an effect, difference, or relationship.\n",
    "Example: \"There is a difference in test scores between two teaching methods.\"\n",
    "\n",
    "3. Test Statistic:\n",
    "->A numerical value calculated from the sample data that is used to decide whether to reject the null hypothesis.\n",
    "->Common test statistics include z-scores, t-scores, F-scores, and chi-square statistics, depending on the test used.\n",
    "\n",
    "4. P-Value:\n",
    "->The probability of observing the test statistic or something more extreme, assuming the null hypothesis is true.\n",
    "->A smaller p-value indicates stronger evidence against the null hypothesis.\n",
    "\n",
    "5. Significance Level (Œ±):\n",
    "A threshold for the p-value used to decide whether to reject the null hypothesis. Common values are 0.05, 0.01, and 0.10.\n",
    "\n",
    "6. Make a Decision:\n",
    "->Compare the p-value to the significance level (Œ±).\n",
    "->If the p-value ‚â§ Œ±, reject the null hypothesis.\n",
    "->If the p-value > Œ±, fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4effd462-188f-437c-9098-0ee4ff0ac2e1",
   "metadata": {},
   "source": [
    "Type 1 and Type 2 error\n",
    "\n",
    "Reality : Null hypothesis is true r false\n",
    "conclusion : Null hypothesis is true r false\n",
    "\n",
    "Outcome 1 : We reject the null hypothesis in reality it is False (Good)\n",
    "Outcome 2 : We reject the null hypothesis in reality it is True  (Type 1 -Error)\n",
    "Outcome 3 : we accept the null hypothesis in reality it is False (Type 2 -Error)\n",
    "Outcome 4 : We accept the null hypothesis in reality it is True  (Good)\n",
    "\n",
    "Type 1 Error is False Positive\n",
    "Type 2 Error is False Negative \n",
    "\n",
    "A Type I error occurs when the null hypothesis(Ho) is true, but we mistakenly reject it. In other words, it's the incorrect rejection of a true null hypothesis.\n",
    "A Type II error occurs when the null hypothesis(Ho) is false, but we fail to reject it. In other words, it's the failure to reject a false null hypothesis.\n",
    "\n",
    "Ex :\n",
    "1. Test indicates that patient has diseas but in reality they do not \n",
    "2. Test indicates that patient do not have disease but in reality thy do have.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332057c2-0002-4457-b895-5420a518810d",
   "metadata": {},
   "source": [
    "Correlation : Correlation is the statistical measure that describes the strength and direction of linear relationship between two random variables.Correlation is a standardized form of covariance.It is standardized measure and its value ranges from (-1 to +1)\n",
    "+1 indicates a perfect positive linear relationship.\n",
    "‚àí1 indicates a perfect negative linear relationship.\n",
    "0 indicates no linear relationship.\n",
    "\n",
    "Covariance : Covariance measures the direction of the linear relationship between two variables. It indicates whether an increase in one variable corresponds to an increase (positive covariance) or a decrease (negative covariance) in the other variable.Not standardized value depends on unit of variables.\n",
    "->Positive covariance indicates that the two variables tend to move in the same direction.\n",
    "->Negative covariance indicates that the two variables tend to move in opposite directions.\n",
    "->Zero covariance indicates no linear relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c99379-0c87-46a3-a934-24d17c10ac69",
   "metadata": {},
   "source": [
    "PROBABILITY: An action or process that leads 1 or more possible outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5037faf-ad3c-4643-bee5-29c6374751ec",
   "metadata": {},
   "source": [
    "Random variable : It is a variable whose possible values are numerical outcomes of random phenomenon\n",
    "\n",
    "Discrete random variable: which has countable numeric of distinct value \n",
    "Ex: number of students in class \n",
    "\n",
    "Continous random variable : which has infinite number of possible values within given range \n",
    "Ex: height,weight \n",
    "\n",
    "Probability : An action r process that lead 1 or more possible outcomes\n",
    "Ex; Rolling die P(A)=Number of favorable outcome/Total number of outcomes\n",
    "\n",
    "Addition rule ( 'or')\n",
    "Multiple rule ('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5459af-dcf6-44e4-8d81-90b53ae9561a",
   "metadata": {},
   "source": [
    "Probability Distribution : It is a function that provides the probability of occurance of different possible outcomes in an experiment.\n",
    "1.Discrete probability distribution : Burnoulli,Binomail,Poission\n",
    "2.Continuous probability distribution : Normal,uniform ,chisquare\n",
    "\n",
    "1.Bernoulli Distribution : An experiement which takes only 2 possible outcome(success,failure)\n",
    "2.Binomial Distribution : Represent the number of success in a fixed number of independent bernoulli trails.\n",
    "3.Normal(Gaussian)Distribution : It i s a continous random variable with symmetric bell shaped distribution (mean=median=mode).\n",
    "4.Uniform Distribution : Represents a continuous random variables that has equal probability over a given range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc74b08-7ec1-43f0-b5f8-ad16595cdf89",
   "metadata": {},
   "source": [
    "Permutations : Permutations are arrangements of items where the order matters. The number of permutations of a set of n distinct items taken ùëü at a time is given by:\n",
    "P(n,r)=n!/(n-r)!\n",
    "\n",
    "Combination:Combinations are selections of items where the order does not matter. The number of combinations of a set of n distinct items taken r at a time is given by:\n",
    "C(n,r)=n!/r!(n-r)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eab517-80b6-4ab2-80e3-c55e74952903",
   "metadata": {},
   "source": [
    "Bayes theorem:Bayes' Theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis based on new evidence or information.which describes the probability of an event based on prior knowledge of conditions related to the event.\n",
    "\n",
    "P(A/B)=P(B/A)*P(A)/P(B).\n",
    "\n",
    "P(A/B)=Probability of event A given B is true\n",
    "P(A)=Probability of event A\n",
    "P(B)=Probability of event B\n",
    "P(B/A)=Probability of event B given A is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26deb2f-047d-40b7-ac52-736040fa7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutations P(5, 3): 60\n",
      "Combinations C(5, 3): 10\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Number of permutations of 5 items taken 3 at a time\n",
    "n = 5\n",
    "r = 3\n",
    "permutations = math.perm(n, r)\n",
    "print(f\"Permutations P({n}, {r}): {permutations}\")\n",
    "\n",
    "# Number of combinations of 5 items taken 3 at a time\n",
    "combinations = math.comb(n, r)\n",
    "print(f\"Combinations C({n}, {r}): {combinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5b1a4-07ee-4e5b-bd13-28faa393c958",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cca7504-e2ca-4759-87af-88f956ae852c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f6055b0-3eb0-4d48-80af-0d449b20dc2d",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06680e2d-0ee0-4a72-87d4-3e81a45ab4d3",
   "metadata": {},
   "source": [
    "Standardization : It is a feature scaling  data preprocesing technique used in ml and statistics,it is used when the feature of the dataset are on different scale to bring them to a common scale.It transforms the data to have mean of 0 and std of 1.\n",
    "using z_score.The algorithm assumes that the data is normally distributed(Linear,logistic,SVM)\n",
    "\n",
    "z_score=x-mean/std\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "data=scaler.fit_transform(data)\n",
    "\n",
    "When to use standardization :\n",
    "1. Gradient Descent-Based Algorithms: Algorithms like linear regression, logistic regression, and neural networks.\n",
    "2. Distance-Based Algorithms: Algorithms like k-nearest neighbors (KNN) and support vector machines (SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d473711-b159-4a79-a35e-48f1de6ae1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[1. 2.]\n",
      " [2. 3.]\n",
      " [3. 4.]\n",
      " [4. 5.]\n",
      " [5. 6.]]\n",
      "Standardized Data:\n",
      " [[-1.41421356 -1.41421356]\n",
      " [-0.70710678 -0.70710678]\n",
      " [ 0.          0.        ]\n",
      " [ 0.70710678  0.70710678]\n",
      " [ 1.41421356  1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "data = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0], [5.0, 6.0]])\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Standardized Data:\\n\", standardized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65dcd9-ae7a-4a97-b115-53c557f04077",
   "metadata": {},
   "source": [
    "Normalization : It is a feature scaling  data preprocessing technique used in ml and statistics.It rescales the values of feature to range of (0,1) as min-max-scaling.Normalization is used when you want to ensure that the data fits within a specific range,particulerly in algorithm that do not make assumption about the distribution of data.It is especially useful when features in a dataset have different units or scales. Normalization ensures that each feature contributes equally to the model and can improve the performance of algorithms that are sensitive to feature scaling.\n",
    "\n",
    "x=(x-min(x))/(max(x)-min(x))\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data=scaler.fit_transform(data)\n",
    "\n",
    "When to use Normalization :\n",
    "1. Gradient Descent-Based Algorithms: Algorithms like linear regression, logistic regression, and neural networks.\n",
    "2. Distance-Based Algorithms:Algorithms like k-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "3. Algorithms Sensitive to Scale: Algorithms like k-nearest neighbors (KNN), support vector machines (SVM), and gradient descent-based methods benefit from normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464d16b7-1b62-4b12-843e-9f6369203455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    Feature1  Feature2\n",
      "0      10.0     100.0\n",
      "1      20.0     200.0\n",
      "2      30.0     300.0\n",
      "3      40.0     400.0\n",
      "4      50.0     500.0\n",
      "\n",
      "Normalized Data:\n",
      "    Feature1  Feature2\n",
      "0      0.00      0.00\n",
      "1      0.25      0.25\n",
      "2      0.50      0.50\n",
      "3      0.75      0.75\n",
      "4      1.00      1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({\n",
    "    'Feature1': [10.0, 20.0, 30.0, 40.0, 50.0],\n",
    "    'Feature2': [100.0, 200.0, 300.0, 400.0, 500.0]\n",
    "})\n",
    "\n",
    "# Display the original data\n",
    "print(\"Original Data:\\n\", data)\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame for better readability\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=data.columns)\n",
    "\n",
    "# Display the normalized data\n",
    "print(\"\\nNormalized Data:\\n\", normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f1984-4ce8-459e-8092-e4812f581989",
   "metadata": {},
   "source": [
    "PCA : Principal Component Analysis  is a statistical technique used for dimensionality reduction in data analysis and machine learning.reducing the number of features while retain the essential information.which is used to transform a high-dimentional dataset to lower dimentional without retailing its original data.\n",
    "\n",
    "Application of PCA :\n",
    "1. Dimensionality Reduction: Reduces the number of variables in high-dimensional datasets while retaining as much variance as possible.\n",
    "2. Data Visualization: Visualizes high-dimensional data in lower-dimensional space (often 2D or 3D) to explore patterns and relationships.\n",
    "3. Noise Filtering: PCA can filter out noise by retaining only the principal components with significant eigenvalues.\n",
    "4. Feature Extraction: PCA can be used as a feature extraction technique for machine learning algorithms by transforming high-dimensional features into a lower-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff132cd9-c25d-4f19-ab50-723534223c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components (Eigenvectors):\n",
      " [[-0.70710678 -0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "\n",
      "Explained Variance Ratio: [1.00000000e+00 3.69778549e-33]\n",
      "\n",
      "Transformed Data (PCA):\n",
      " [[ 2.00000000e+00  1.48952049e-16]\n",
      " [ 1.00000000e+00 -4.96506831e-17]\n",
      " [-0.00000000e+00  0.00000000e+00]\n",
      " [-1.00000000e+00  4.96506831e-17]\n",
      " [-2.00000000e+00  9.93013661e-17]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "data = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=2)  # You can specify the number of principal components to keep\n",
    "\n",
    "# Fit PCA and transform the data\n",
    "data_pca = pca.fit_transform(data_standardized)\n",
    "\n",
    "# Print the principal components\n",
    "print(\"Principal Components (Eigenvectors):\\n\", pca.components_)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"\\nExplained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the transformed data (reduced-dimensional representation)\n",
    "print(\"\\nTransformed Data (PCA):\\n\", data_pca)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
